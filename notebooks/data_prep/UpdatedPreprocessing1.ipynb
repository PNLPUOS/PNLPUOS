{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Subir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Subir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Subir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install porter2stemmer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from html.parser import HTMLParser\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "#from porter2stemmer import Porter2Stemmer\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwordList = stopwords.words('english')\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>report_grouping</th>\n",
       "      <th>question_text</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Large Department</td>\n",
       "      <td>Please tell us what is working well.</td>\n",
       "      <td>we do what our customers need, we communicate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Large Department</td>\n",
       "      <td>Please tell us what is working well.</td>\n",
       "      <td>Customs business development continues to grow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Large Department</td>\n",
       "      <td>Please tell us what is working well.</td>\n",
       "      <td>I think the team work hard, are committed to c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Large Department</td>\n",
       "      <td>Please tell us what is working well.</td>\n",
       "      <td>Overall working towards a customer centric env...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Large Department</td>\n",
       "      <td>Please tell us what is working well.</td>\n",
       "      <td>Customer centricity is a growing culture in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Large Department</td>\n",
       "      <td>Please tell us what is working well.</td>\n",
       "      <td>•Develop a comfortable rapport with clients an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Large Department</td>\n",
       "      <td>Please tell us what is working well.</td>\n",
       "      <td>THE CUSTOMER IS THE CENTER.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Large Department</td>\n",
       "      <td>Please tell us what is working well.</td>\n",
       "      <td>we are usually seeking customer satisfaction, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Large Department</td>\n",
       "      <td>Please tell us what is working well.</td>\n",
       "      <td>Alignment between regional office and country ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Large Department</td>\n",
       "      <td>Please tell us what is working well.</td>\n",
       "      <td>innovation, customer relations ship and custom...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    report_grouping                         question_text  \\\n",
       "0  Large Department  Please tell us what is working well.   \n",
       "1  Large Department  Please tell us what is working well.   \n",
       "2  Large Department  Please tell us what is working well.   \n",
       "3  Large Department  Please tell us what is working well.   \n",
       "4  Large Department  Please tell us what is working well.   \n",
       "5  Large Department  Please tell us what is working well.   \n",
       "6  Large Department  Please tell us what is working well.   \n",
       "7  Large Department  Please tell us what is working well.   \n",
       "8  Large Department  Please tell us what is working well.   \n",
       "9  Large Department  Please tell us what is working well.   \n",
       "\n",
       "                                            comments  \n",
       "0  we do what our customers need, we communicate ...  \n",
       "1  Customs business development continues to grow...  \n",
       "2  I think the team work hard, are committed to c...  \n",
       "3  Overall working towards a customer centric env...  \n",
       "4  Customer centricity is a growing culture in th...  \n",
       "5  •Develop a comfortable rapport with clients an...  \n",
       "6                        THE CUSTOMER IS THE CENTER.  \n",
       "7  we are usually seeking customer satisfaction, ...  \n",
       "8  Alignment between regional office and country ...  \n",
       "9  innovation, customer relations ship and custom...  "
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('pnlp_data_en.csv', delimiter=';')\n",
    "df.dropna()\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "# rename columns for easier usability\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "# preview\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apostrophe Dictionary\n",
    "apostrophe = {\n",
    "    \"aren't\" : \"are not\",\n",
    "    \"can't\" : \"cannot\",\n",
    "    \"couldn't\" : \"could not\",\n",
    "    \"didn't\" : \"did not\",\n",
    "    \"doesn't\" : \"does not\",\n",
    "    \"don't\" : \"do not\",\n",
    "    \"hadn't\" : \"had not\",\n",
    "    \"hasn't\" : \"has not\",\n",
    "    \"haven't\" : \"have not\",\n",
    "    \"he'd\" : \"he would\",\n",
    "    \"he'll\" : \"he will\",\n",
    "    \"he's\" : \"he is\",\n",
    "    \"i'd\" : \"I would\",\n",
    "    \"i'd\" : \"I had\",\n",
    "    \"i'll\" : \"I will\",\n",
    "    \"i'm\" : \"I am\",\n",
    "    \"isn't\" : \"is not\",\n",
    "    \"it's\" : \"it is\",\n",
    "    \"it'll\":\"it will\",\n",
    "    \"i've\" : \"I have\",\n",
    "    \"let's\" : \"let us\",\n",
    "    \"mightn't\" : \"might not\",\n",
    "    \"mustn't\" : \"must not\",\n",
    "    \"shan't\" : \"shall not\",\n",
    "    \"she'd\" : \"she would\",\n",
    "    \"she'll\" : \"she will\",\n",
    "    \"she's\" : \"she is\",\n",
    "    \"shouldn't\" : \"should not\",\n",
    "    \"that's\" : \"that is\",\n",
    "    \"there's\" : \"there is\",\n",
    "    \"they'd\" : \"they would\",\n",
    "    \"they'll\" : \"they will\",\n",
    "    \"they're\" : \"they are\",\n",
    "    \"they've\" : \"they have\",\n",
    "    \"we'd\" : \"we would\",\n",
    "    \"we're\" : \"we are\",\n",
    "    \"weren't\" : \"were not\",\n",
    "    \"we've\" : \"we have\",\n",
    "    \"what'll\" : \"what will\",\n",
    "    \"what're\" : \"what are\",\n",
    "    \"what's\" : \"what is\",\n",
    "    \"what've\" : \"what have\",\n",
    "    \"where's\" : \"where is\",\n",
    "    \"who'd\" : \"who would\",\n",
    "    \"who'll\" : \"who will\",\n",
    "    \"who're\" : \"who are\",\n",
    "    \"who's\" : \"who is\",\n",
    "    \"who've\" : \"who have\",\n",
    "    \"won't\" : \"will not\",\n",
    "    \"wouldn't\" : \"would not\",\n",
    "    \"you'd\" : \"you would\",\n",
    "    \"you'll\" : \"you will\",\n",
    "    \"you're\" : \"you are\",\n",
    "    \"you've\" : \"you have\",\n",
    "    \"'re\": \" are\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'll\":\" will\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"'s\": \"is\",\n",
    "    \"'re\": \"are\"\n",
    "}\n",
    "\n",
    "short_words = {\n",
    "\"121\": \"one to one\",\n",
    "\"a/s/l\": \"age, sex, location\",\n",
    "\"adn\": \"any day now\",\n",
    "\"afaik\": \"as far as I know\",\n",
    "\"afk\": \"away from keyboard\",\n",
    "\"aight\": \"alright\",\n",
    "\"alol\": \"actually laughing out loud\",\n",
    "\"b4\": \"before\",\n",
    "\"b4n\": \"bye for now\",\n",
    "\"bak\": \"back at the keyboard\",\n",
    "\"bf\": \"boyfriend\",\n",
    "\"bff\": \"best friends forever\",\n",
    "\"bfn\": \"bye for now\",\n",
    "\"bg\": \"big grin\",\n",
    "\"bta\": \"but then again\",\n",
    "\"btw\": \"by the way\",\n",
    "\"cid\": \"crying in disgrace\",\n",
    "\"cnp\": \"continued in my next post\",\n",
    "\"cp\": \"chat post\",\n",
    "\"cu\": \"see you\",\n",
    "\"cul\": \"see you later\",\n",
    "\"cul8r\": \"see you later\",\n",
    "\"cya\": \"bye\",\n",
    "\"cyo\": \"see you online\",\n",
    "\"dbau\": \"doing business as usual\",\n",
    "\"fud\": \"fear, uncertainty, and doubt\",\n",
    "\"fwiw\": \"for what it's worth\",\n",
    "\"fyi\": \"for your information\",\n",
    "\"g\": \"grin\",\n",
    "\"g2g\": \"got to go\",\n",
    "\"ga\": \"go ahead\",\n",
    "\"gal\": \"get a life\",\n",
    "\"gf\": \"girlfriend\",\n",
    "\"gfn\": \"gone for now\",\n",
    "\"gmbo\": \"giggling my butt off\",\n",
    "\"gmta\": \"great minds think alike\",\n",
    "\"h8\": \"hate\",\n",
    "\"hagn\": \"have a good night\",\n",
    "\"hdop\": \"help delete online predators\",\n",
    "\"hhis\": \"hanging head in shame\",\n",
    "\"iac\": \"in any case\",\n",
    "\"ianal\": \"I am not a lawyer\",\n",
    "\"ic\": \"I see\",\n",
    "\"idk\": \"I don't know\",\n",
    "\"imao\": \"in my arrogant opinion\",\n",
    "\"imnsho\": \"in my not so humble opinion\",\n",
    "\"imo\": \"in my opinion\",\n",
    "\"iow\": \"in other words\",\n",
    "\"ipn\": \"I’m posting naked\",\n",
    "\"irl\": \"in real life\",\n",
    "\"jk\": \"just kidding\",\n",
    "\"l8r\": \"later\",\n",
    "\"ld\": \"later, dude\",\n",
    "\"ldr\": \"long distance relationship\",\n",
    "\"llta\": \"lots and lots of thunderous applause\",\n",
    "\"lmao\": \"laugh my ass off\",\n",
    "\"lmirl\": \"let's meet in real life\",\n",
    "\"lol\": \"laugh out loud\",\n",
    "\"ltr\": \"longterm relationship\",\n",
    "\"lulab\": \"love you like a brother\",\n",
    "\"lulas\": \"love you like a sister\",\n",
    "\"luv\": \"love\",\n",
    "\"m/f\": \"male or female\",\n",
    "\"m8\": \"mate\",\n",
    "\"milf\": \"mother I would like to fuck\",\n",
    "\"oll\": \"online love\",\n",
    "\"omg\": \"oh my god\",\n",
    "\"otoh\": \"on the other hand\",\n",
    "\"pir\": \"parent in room\",\n",
    "\"ppl\": \"people\",\n",
    "\"r\": \"are\",\n",
    "\"rofl\": \"roll on the floor laughing\",\n",
    "\"rpg\": \"role playing games\",\n",
    "\"ru\": \"are you\",\n",
    "\"shid\": \"slaps head in disgust\",\n",
    "\"somy\": \"sick of me yet\",\n",
    "\"sot\": \"short of time\",\n",
    "\"thanx\": \"thanks\",\n",
    "\"thx\": \"thanks\",\n",
    "\"ttyl\": \"talk to you later\",\n",
    "\"u\": \"you\",\n",
    "\"ur\": \"you are\",\n",
    "\"uw\": \"you’re welcome\",\n",
    "\"wb\": \"welcome back\",\n",
    "\"wfm\": \"works for me\",\n",
    "\"wibni\": \"wouldn't it be nice if\",\n",
    "\"wtf\": \"what the fuck\",\n",
    "\"wtg\": \"way to go\",\n",
    "\"wtgp\": \"want to go private\",\n",
    "\"ym\": \"young man\",\n",
    "\"gr8\": \"great\"\n",
    "}\n",
    "\n",
    "def apos_short_dict(text, dictionary):\n",
    "    for word in text.split():\n",
    "        if word.lower() in dictionary:\n",
    "            if word.lower() in text.split():\n",
    "                text = text.replace(word, dictionary[word.lower()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>report_grouping</th>\n",
       "      <th>question_text</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Large Department</td>\n",
       "      <td>Please tell us what is working well.</td>\n",
       "      <td>[custom, need, commun, aperiod]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Large Department</td>\n",
       "      <td>Please tell us what is working well.</td>\n",
       "      <td>[custom, busi, develop, continu, grow, expand,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Large Department</td>\n",
       "      <td>Please tell us what is working well.</td>\n",
       "      <td>[think, team, work, hard, commit, continu, imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Large Department</td>\n",
       "      <td>Please tell us what is working well.</td>\n",
       "      <td>[overal, work, toward, custom, centric, enviro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Large Department</td>\n",
       "      <td>Please tell us what is working well.</td>\n",
       "      <td>[custom, centric, grow, cultur, compani, creat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    report_grouping                         question_text  \\\n",
       "0  Large Department  Please tell us what is working well.   \n",
       "1  Large Department  Please tell us what is working well.   \n",
       "2  Large Department  Please tell us what is working well.   \n",
       "3  Large Department  Please tell us what is working well.   \n",
       "4  Large Department  Please tell us what is working well.   \n",
       "\n",
       "                                            comments  \n",
       "0                    [custom, need, commun, aperiod]  \n",
       "1  [custom, busi, develop, continu, grow, expand,...  \n",
       "2  [think, team, work, hard, commit, continu, imp...  \n",
       "3  [overal, work, toward, custom, centric, enviro...  \n",
       "4  [custom, centric, grow, cultur, compani, creat...  "
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_html(txt):\n",
    "    '''Remove HTML'''\n",
    "    txt = BeautifulSoup(txt, 'lxml')\n",
    "    return txt.get_text()\n",
    "\n",
    "def remove_punctuation(surveyText):\n",
    "    return \"\".join([i for i in surveyText if i not in string.punctuation])\n",
    "\n",
    "def remove_stopwords(surveyText):\n",
    "    return [w for w in surveyText if w not in stopwordList]\n",
    "\n",
    "def word_lemmatizer(surveyText):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    w_tokenizer = WhitespaceTokenizer()\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(surveyText)]\n",
    "\n",
    "def word_stemmer(surveyText):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(i) for i in surveyText]\n",
    "    \n",
    "def preprocessing(txt, punctuation= False, tokenize= False, stopwords= False, correct_apos= False, \n",
    "                  shortWords= False, specialCharacter= False, numbers= False, singleChar= False,\n",
    "                 lematization= False, stemming= False):\n",
    "    \n",
    "    cleanedTxt = txt.apply(lambda x: remove_html(x))\n",
    "    \n",
    "    if punctuation:\n",
    "        cleanedTxt = cleanedTxt.apply(lambda x:remove_punctuation(x))\n",
    "        \n",
    "    if tokenize:\n",
    "        cleanedTxt = cleanedTxt.apply(lambda x:word_tokenize(x.lower()))\n",
    "        \n",
    "    if stopwords:\n",
    "        cleanedTxt = cleanedTxt.apply(lambda x: remove_stopwords(x))\n",
    "        \n",
    "    if correct_apos:\n",
    "        cleanedTxt = cleanedTxt.apply(lambda x: apos_short_dict(str(x),apostrophe))\n",
    "        \n",
    "    if shortWords:\n",
    "        cleanedTxt = cleanedTxt.apply(lambda x: apos_short_dict(str(x),short_words))\n",
    "    \n",
    "    if specialCharacter:\n",
    "        '''Replacing Special Characters with space'''\n",
    "        cleanedTxt = cleanedTxt.apply(lambda x: re.sub(r'[^a-zA-Z0-9]',' ',str(x)))\n",
    "    \n",
    "    if numbers:\n",
    "        '''Replacing Numbers with space'''\n",
    "        cleanedTxt = cleanedTxt.apply(lambda x: re.sub(r'[^a-zA-Z]',' ',x))\n",
    "        \n",
    "    if singleChar:\n",
    "        '''Removing words whom length is one'''\n",
    "        cleanedTxt = cleanedTxt.apply(lambda x: ' '.join([w for w in x.split() if len(w)>1]))\n",
    "    \n",
    "    if lematization:\n",
    "        cleanedTxt = cleanedTxt.apply(lambda x: word_lemmatizer(x))\n",
    "        \n",
    "    if stemming:\n",
    "        cleanedTxt = cleanedTxt.apply(lambda x: word_stemmer(x))\n",
    "        #pass\n",
    "    \n",
    "    return cleanedTxt\n",
    "      \n",
    "df['comments'] = preprocessing(df['comments'], punctuation= True, tokenize= True, stopwords= True, \n",
    "                                correct_apos= True, shortWords= True, specialCharacter= True, \n",
    "                               numbers= True, singleChar= True, lematization= True, stemming= True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>report_grouping</th>\n",
       "      <th>question_text</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Large Department</td>\n",
       "      <td>Please tell us what is working well.</td>\n",
       "      <td>custom need commun aperiod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Large Department</td>\n",
       "      <td>Please tell us what is working well.</td>\n",
       "      <td>custom busi develop continu grow expand use in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Large Department</td>\n",
       "      <td>Please tell us what is working well.</td>\n",
       "      <td>think team work hard commit continu improv goo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Large Department</td>\n",
       "      <td>Please tell us what is working well.</td>\n",
       "      <td>overal work toward custom centric environ work...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Large Department</td>\n",
       "      <td>Please tell us what is working well.</td>\n",
       "      <td>custom centric grow cultur compani creat posit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    report_grouping                         question_text  \\\n",
       "0  Large Department  Please tell us what is working well.   \n",
       "1  Large Department  Please tell us what is working well.   \n",
       "2  Large Department  Please tell us what is working well.   \n",
       "3  Large Department  Please tell us what is working well.   \n",
       "4  Large Department  Please tell us what is working well.   \n",
       "\n",
       "                                            comments  \n",
       "0                         custom need commun aperiod  \n",
       "1  custom busi develop continu grow expand use in...  \n",
       "2  think team work hard commit continu improv goo...  \n",
       "3  overal work toward custom centric environ work...  \n",
       "4  custom centric grow cultur compani creat posit...  "
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# de-tokenization\n",
    "detokenizer = []\n",
    "for i in range(len(df)):\n",
    "    t = ' '.join(df['comments'][i]) # tokenized text\n",
    "    detokenizer.append(t)\n",
    "\n",
    "df['comments'] = detokenizer\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
