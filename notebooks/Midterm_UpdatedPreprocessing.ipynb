{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "Midterm_UpdatedPreprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8PWGGuNsmFB",
        "colab_type": "code",
        "outputId": "46eff332-7cf8-4cab-f6aa-928f7f1cbdb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from bs4 import BeautifulSoup\n",
        "from html.parser import HTMLParser\n",
        "import string\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk, pos_tag_sents\n",
        "from nltk.corpus import stopwords\n",
        "#from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "#from porter2stemmer import Porter2Stemmer\n",
        "import re\n",
        "from nltk.util import ngrams\n",
        "\n",
        "# for spell correction news dataset\n",
        "from collections import Counter\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "#from SpellCorrector import SpellCorrector\n",
        "\n",
        "#from spellchecker import SpellChecker\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stopwordList = stopwords.words('english')\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tarHrVrnsmFI",
        "colab_type": "code",
        "outputId": "9115e9d2-5fec-4e8e-f574-bd8a1ac8880b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "df = pd.read_csv('pnlp_data_en.csv', delimiter=';')\n",
        "df.dropna()\n",
        "pd.set_option('display.max_columns', 50)\n",
        "pd.set_option('display.max_rows', 50)\n",
        "pd.set_option('display.max_colwidth', 50)\n",
        "\n",
        "# rename columns for easier usability\n",
        "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
        "del df['report_grouping']\n",
        "del df['question_text']\n",
        "# preview\n",
        "df.head(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comments</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>we do what our customers need, we communicate ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Customs business development continues to grow...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I think the team work hard, are committed to c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Overall working towards a customer centric env...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Customer centricity is a growing culture in th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>•Develop a comfortable rapport with clients an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>THE CUSTOMER IS THE CENTER.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>we are usually seeking customer satisfaction, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Alignment between regional office and country ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>innovation, customer relations ship and custom...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            comments\n",
              "0  we do what our customers need, we communicate ...\n",
              "1  Customs business development continues to grow...\n",
              "2  I think the team work hard, are committed to c...\n",
              "3  Overall working towards a customer centric env...\n",
              "4  Customer centricity is a growing culture in th...\n",
              "5  •Develop a comfortable rapport with clients an...\n",
              "6                        THE CUSTOMER IS THE CENTER.\n",
              "7  we are usually seeking customer satisfaction, ...\n",
              "8  Alignment between regional office and country ...\n",
              "9  innovation, customer relations ship and custom..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYTZSrErsmFM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Apostrophe Dictionary and if have more words in mind, please add it in the bottom\n",
        "apostrophe = {\n",
        "    \"aren't\" : \"are not\",\n",
        "    \"can't\" : \"cannot\",\n",
        "    \"couldn't\" : \"could not\",\n",
        "    \"didn't\" : \"did not\",\n",
        "    \"doesn't\" : \"does not\",\n",
        "    \"don't\" : \"do not\",\n",
        "    \"hadn't\" : \"had not\",\n",
        "    \"hasn't\" : \"has not\",\n",
        "    \"haven't\" : \"have not\",\n",
        "    \"he'd\" : \"he would\",\n",
        "    \"he'll\" : \"he will\",\n",
        "    \"he's\" : \"he is\",\n",
        "    \"i'd\" : \"I would\",\n",
        "    \"i'd\" : \"I had\",\n",
        "    \"i'll\" : \"I will\",\n",
        "    \"i'm\" : \"I am\",\n",
        "    \"isn't\" : \"is not\",\n",
        "    \"it's\" : \"it is\",\n",
        "    \"it'll\":\"it will\",\n",
        "    \"i've\" : \"I have\",\n",
        "    \"let's\" : \"let us\",\n",
        "    \"mightn't\" : \"might not\",\n",
        "    \"mustn't\" : \"must not\",\n",
        "    \"shan't\" : \"shall not\",\n",
        "    \"she'd\" : \"she would\",\n",
        "    \"she'll\" : \"she will\",\n",
        "    \"she's\" : \"she is\",\n",
        "    \"shouldn't\" : \"should not\",\n",
        "    \"that's\" : \"that is\",\n",
        "    \"there's\" : \"there is\",\n",
        "    \"they'd\" : \"they would\",\n",
        "    \"they'll\" : \"they will\",\n",
        "    \"they're\" : \"they are\",\n",
        "    \"they've\" : \"they have\",\n",
        "    \"we'd\" : \"we would\",\n",
        "    \"we're\" : \"we are\",\n",
        "    \"weren't\" : \"were not\",\n",
        "    \"we've\" : \"we have\",\n",
        "    \"what'll\" : \"what will\",\n",
        "    \"what're\" : \"what are\",\n",
        "    \"what's\" : \"what is\",\n",
        "    \"what've\" : \"what have\",\n",
        "    \"where's\" : \"where is\",\n",
        "    \"who'd\" : \"who would\",\n",
        "    \"who'll\" : \"who will\",\n",
        "    \"who're\" : \"who are\",\n",
        "    \"who's\" : \"who is\",\n",
        "    \"who've\" : \"who have\",\n",
        "    \"won't\" : \"will not\",\n",
        "    \"wouldn't\" : \"would not\",\n",
        "    \"you'd\" : \"you would\",\n",
        "    \"you'll\" : \"you will\",\n",
        "    \"you're\" : \"you are\",\n",
        "    \"you've\" : \"you have\",\n",
        "    \"'re\": \" are\",\n",
        "    \"wasn't\": \"was not\",\n",
        "    \"we'll\":\" will\",\n",
        "    \"didn't\": \"did not\",\n",
        "    \"'s\": \"is\",\n",
        "    \"'re\": \"are\"\n",
        "}\n",
        "\n",
        "#Short words dictionary and if have more words in mind, please add it in the bottom\n",
        "short_words = {\n",
        "\"121\": \"one to one\",\n",
        "\"a/s/l\": \"age, sex, location\",\n",
        "\"adn\": \"any day now\",\n",
        "\"afaik\": \"as far as I know\",\n",
        "\"afk\": \"away from keyboard\",\n",
        "\"aight\": \"alright\",\n",
        "\"alol\": \"actually laughing out loud\",\n",
        "\"b4\": \"before\",\n",
        "\"b4n\": \"bye for now\",\n",
        "\"bak\": \"back at the keyboard\",\n",
        "\"bf\": \"boyfriend\",\n",
        "\"bff\": \"best friends forever\",\n",
        "\"bfn\": \"bye for now\",\n",
        "\"bg\": \"big grin\",\n",
        "\"bta\": \"but then again\",\n",
        "\"btw\": \"by the way\",\n",
        "\"cid\": \"crying in disgrace\",\n",
        "\"cnp\": \"continued in my next post\",\n",
        "\"cp\": \"chat post\",\n",
        "\"cu\": \"see you\",\n",
        "\"cul\": \"see you later\",\n",
        "\"cul8r\": \"see you later\",\n",
        "\"cya\": \"bye\",\n",
        "\"cyo\": \"see you online\",\n",
        "\"dbau\": \"doing business as usual\",\n",
        "\"fud\": \"fear, uncertainty, and doubt\",\n",
        "\"fwiw\": \"for what it's worth\",\n",
        "\"fyi\": \"for your information\",\n",
        "\"g\": \"grin\",\n",
        "\"g2g\": \"got to go\",\n",
        "\"ga\": \"go ahead\",\n",
        "\"gal\": \"get a life\",\n",
        "\"gf\": \"girlfriend\",\n",
        "\"gfn\": \"gone for now\",\n",
        "\"gmbo\": \"giggling my butt off\",\n",
        "\"gmta\": \"great minds think alike\",\n",
        "\"h8\": \"hate\",\n",
        "\"hagn\": \"have a good night\",\n",
        "\"hdop\": \"help delete online predators\",\n",
        "\"hhis\": \"hanging head in shame\",\n",
        "\"iac\": \"in any case\",\n",
        "\"ianal\": \"I am not a lawyer\",\n",
        "\"ic\": \"I see\",\n",
        "\"idk\": \"I don't know\",\n",
        "\"imao\": \"in my arrogant opinion\",\n",
        "\"imnsho\": \"in my not so humble opinion\",\n",
        "\"imo\": \"in my opinion\",\n",
        "\"iow\": \"in other words\",\n",
        "\"ipn\": \"I’m posting naked\",\n",
        "\"irl\": \"in real life\",\n",
        "\"jk\": \"just kidding\",\n",
        "\"l8r\": \"later\",\n",
        "\"ld\": \"later, dude\",\n",
        "\"ldr\": \"long distance relationship\",\n",
        "\"llta\": \"lots and lots of thunderous applause\",\n",
        "\"lmao\": \"laugh my ass off\",\n",
        "\"lmirl\": \"let's meet in real life\",\n",
        "\"lol\": \"laugh out loud\",\n",
        "\"ltr\": \"longterm relationship\",\n",
        "\"lulab\": \"love you like a brother\",\n",
        "\"lulas\": \"love you like a sister\",\n",
        "\"luv\": \"love\",\n",
        "\"m/f\": \"male or female\",\n",
        "\"m8\": \"mate\",\n",
        "\"milf\": \"mother I would like to fuck\",\n",
        "\"oll\": \"online love\",\n",
        "\"omg\": \"oh my god\",\n",
        "\"otoh\": \"on the other hand\",\n",
        "\"pir\": \"parent in room\",\n",
        "\"ppl\": \"people\",\n",
        "\"r\": \"are\",\n",
        "\"rofl\": \"roll on the floor laughing\",\n",
        "\"rpg\": \"role playing games\",\n",
        "\"ru\": \"are you\",\n",
        "\"shid\": \"slaps head in disgust\",\n",
        "\"somy\": \"sick of me yet\",\n",
        "\"sot\": \"short of time\",\n",
        "\"thanx\": \"thanks\",\n",
        "\"thx\": \"thanks\",\n",
        "\"ttyl\": \"talk to you later\",\n",
        "\"u\": \"you\",\n",
        "\"ur\": \"you are\",\n",
        "\"uw\": \"you’re welcome\",\n",
        "\"wb\": \"welcome back\",\n",
        "\"wfm\": \"works for me\",\n",
        "\"wibni\": \"wouldn't it be nice if\",\n",
        "\"wtf\": \"what the fuck\",\n",
        "\"wtg\": \"way to go\",\n",
        "\"wtgp\": \"want to go private\",\n",
        "\"ym\": \"young man\",\n",
        "\"gr8\": \"great\"\n",
        "}\n",
        "\n",
        "def apos_short_dict(text, dictionary):\n",
        "    for word in text.split():\n",
        "        if word.lower() in dictionary:\n",
        "            if word.lower() in text.split():\n",
        "                text = text.replace(word, dictionary[word.lower()])\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XSox5oAsmFP",
        "colab_type": "code",
        "outputId": "77eacad3-2e75-405f-8f2d-9e80f6700a17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "def remove_html(txt):\n",
        "    '''Remove HTML'''\n",
        "    txt = BeautifulSoup(txt, 'lxml')\n",
        "    return txt.get_text()\n",
        "\n",
        "def remove_punctuation(surveyText):\n",
        "    '''Remove any punctuation'''\n",
        "    return \"\".join([i for i in surveyText if i not in string.punctuation])\n",
        "\n",
        "def remove_stopwords(surveyText):\n",
        "    '''Remove stop words'''\n",
        "    return [w for w in surveyText if w not in stopwordList]\n",
        "\n",
        "def word_lemmatizer(surveyText):\n",
        "    '''Lemmatize words'''\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    w_tokenizer = WhitespaceTokenizer()\n",
        "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(surveyText)]\n",
        "\n",
        "def word_stemmer(surveyText):\n",
        "    '''Stem word'''\n",
        "    stemmer = PorterStemmer()\n",
        "    return [stemmer.stem(i) for i in surveyText]\n",
        "\n",
        "# Function to generate n-grams from sentences.\n",
        "def extract_ngrams(txt, num=3):\n",
        "    n_grams = ngrams(nltk.word_tokenize(txt), num)\n",
        "    return [ ' '.join(grams) for grams in n_grams]\n",
        "    \n",
        "'''\n",
        "def pos_tags(txt):\n",
        "  \n",
        "  tokenized = sent_tokenize(txt) \n",
        "  for i in tokenized: \n",
        "      \n",
        "    # Word tokenizers is used to find the words  \n",
        "    # and punctuation in a string \n",
        "    wordsList = nltk.word_tokenize(i) \n",
        "  \n",
        "    # removing stop words from wordList \n",
        "    wordsList = [w for w in wordsList if not w in stop_words]  \n",
        "  \n",
        "    #  Using a Tagger. Which is part-of-speech  \n",
        "    # tagger or POS-tagger.  \n",
        "    tagged = nltk.pos_tag(wordsList) \n",
        "    return [ ' '.join(tags) for tags in tagged]\n",
        "    #print(tagged)\n",
        "\n",
        "'''\n",
        "\n",
        "'''\n",
        "    Control the parameter by putting value of TRUE or FALSE according to requirements. \n",
        "    Args : txt - Provided text for preprocessing\n",
        "            punctuation - Remove all punctuation, Initially value = False\n",
        "            tokenize - Splitting long text into smaller lines\n",
        "            stopwords - Remove such words which does not have much meaning to a line of text\n",
        "            correct_apos - Remove apostrophe\n",
        "            shortwords - Convert any short word to full meaningfull word\n",
        "            specialCharacter - Replace all specialCharacter\n",
        "            numbers - Remove numbers\n",
        "            singleChar - Removing words whom length is one\n",
        "            lematization - Lematize text\n",
        "            stemming - Stemming any text\n",
        "'''\n",
        "def preprocessing(txt, punctuation= False, tokenize= False, stopwords= False, correct_apos= False, \n",
        "                  shortWords= False, specialCharacter= False, numbers= False, singleChar= False,\n",
        "                 lematization= False, stemming= False, ngrams=False):\n",
        "    \n",
        "    cleanedTxt = txt.apply(lambda x: remove_html(x))\n",
        "    \n",
        "    if punctuation:\n",
        "        cleanedTxt = cleanedTxt.apply(lambda x:remove_punctuation(x))\n",
        "    \n",
        "    #if spellCheck:\n",
        "        #cleanedTxt = cleanedTxt.apply(lambda x: spell_correction(x))\n",
        "        \n",
        "    if tokenize:\n",
        "        cleanedTxt = cleanedTxt.apply(lambda x:word_tokenize(x.lower()))\n",
        "        \n",
        "    if stopwords:\n",
        "        cleanedTxt = cleanedTxt.apply(lambda x: remove_stopwords(x))\n",
        "        \n",
        "    if correct_apos:\n",
        "        cleanedTxt = cleanedTxt.apply(lambda x: apos_short_dict(str(x),apostrophe))\n",
        "        \n",
        "    if shortWords:\n",
        "        cleanedTxt = cleanedTxt.apply(lambda x: apos_short_dict(str(x),short_words))\n",
        "    \n",
        "    if specialCharacter:\n",
        "        '''Replacing Special Characters with space'''\n",
        "        cleanedTxt = cleanedTxt.apply(lambda x: re.sub(r'[^a-zA-Z0-9]',' ',str(x)))\n",
        "    \n",
        "    if numbers:\n",
        "        '''Replacing Numbers with space'''\n",
        "        cleanedTxt = cleanedTxt.apply(lambda x: re.sub(r'[^a-zA-Z]',' ',x))\n",
        "        \n",
        "    if singleChar:\n",
        "        '''Removing words whom length is one'''\n",
        "        cleanedTxt = cleanedTxt.apply(lambda x: ' '.join([w for w in x.split() if len(w)>1]))\n",
        "    \n",
        "    if lematization:\n",
        "        cleanedTxt = cleanedTxt.apply(lambda x: word_lemmatizer(x))\n",
        "        \n",
        "    if stemming:\n",
        "        cleanedTxt = cleanedTxt.apply(lambda x: word_stemmer(x))\n",
        "\n",
        "    #if ngrams:\n",
        "        #cleanedTxt = cleanedTxt.apply(lambda x: extract_ngrams(x))\n",
        "    \n",
        "    return cleanedTxt\n",
        "      \n",
        "df['comments'] = preprocessing(df['comments'], punctuation= True, tokenize= True, stopwords= True, \n",
        "                                correct_apos= True, shortWords= True, specialCharacter= True, \n",
        "                               numbers= True, singleChar= True, lematization= False, stemming= False, ngrams= True)\n",
        "df.head(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comments</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>customers need communicate aperiodically</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>customs business development continues grow ex...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>think team work hard committed continuous impr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>overall working towards customer centric envir...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>customer centricity growing culture company cr...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            comments\n",
              "0           customers need communicate aperiodically\n",
              "1  customs business development continues grow ex...\n",
              "2  think team work hard committed continuous impr...\n",
              "3  overall working towards customer centric envir...\n",
              "4  customer centricity growing culture company cr..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeSRXdDP7pOG",
        "colab_type": "code",
        "outputId": "94a5f8f7-fc47-46b3-eec4-1b742cb76d4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Sentence tokenization\n",
        "df['tokenized_sents'] = df['comments'].apply(lambda x: nltk.sent_tokenize(x))\n",
        "df.head(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comments</th>\n",
              "      <th>tokenized_sents</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>customers need communicate aperiodically</td>\n",
              "      <td>[customers need communicate aperiodically]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>customs business development continues grow ex...</td>\n",
              "      <td>[customs business development continues grow e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>think team work hard committed continuous impr...</td>\n",
              "      <td>[think team work hard committed continuous imp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>overall working towards customer centric envir...</td>\n",
              "      <td>[overall working towards customer centric envi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>customer centricity growing culture company cr...</td>\n",
              "      <td>[customer centricity growing culture company c...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            comments  \\\n",
              "0           customers need communicate aperiodically   \n",
              "1  customs business development continues grow ex...   \n",
              "2  think team work hard committed continuous impr...   \n",
              "3  overall working towards customer centric envir...   \n",
              "4  customer centricity growing culture company cr...   \n",
              "\n",
              "                                     tokenized_sents  \n",
              "0         [customers need communicate aperiodically]  \n",
              "1  [customs business development continues grow e...  \n",
              "2  [think team work hard committed continuous imp...  \n",
              "3  [overall working towards customer centric envi...  \n",
              "4  [customer centricity growing culture company c...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JboqeEto7pgW",
        "colab_type": "code",
        "outputId": "38fb3789-94c0-44c0-c340-07c77d9b3b1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "# Word Tokenization\n",
        "df['tokenized_words'] = df['comments'].apply(lambda x: nltk.word_tokenize(x))\n",
        "df.head(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comments</th>\n",
              "      <th>tokenized_sents</th>\n",
              "      <th>tokenized_words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>customers need communicate aperiodically</td>\n",
              "      <td>[customers need communicate aperiodically]</td>\n",
              "      <td>[customers, need, communicate, aperiodically]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>customs business development continues grow ex...</td>\n",
              "      <td>[customs business development continues grow e...</td>\n",
              "      <td>[customs, business, development, continues, gr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>think team work hard committed continuous impr...</td>\n",
              "      <td>[think team work hard committed continuous imp...</td>\n",
              "      <td>[think, team, work, hard, committed, continuou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>overall working towards customer centric envir...</td>\n",
              "      <td>[overall working towards customer centric envi...</td>\n",
              "      <td>[overall, working, towards, customer, centric,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>customer centricity growing culture company cr...</td>\n",
              "      <td>[customer centricity growing culture company c...</td>\n",
              "      <td>[customer, centricity, growing, culture, compa...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            comments  \\\n",
              "0           customers need communicate aperiodically   \n",
              "1  customs business development continues grow ex...   \n",
              "2  think team work hard committed continuous impr...   \n",
              "3  overall working towards customer centric envir...   \n",
              "4  customer centricity growing culture company cr...   \n",
              "\n",
              "                                     tokenized_sents  \\\n",
              "0         [customers need communicate aperiodically]   \n",
              "1  [customs business development continues grow e...   \n",
              "2  [think team work hard committed continuous imp...   \n",
              "3  [overall working towards customer centric envi...   \n",
              "4  [customer centricity growing culture company c...   \n",
              "\n",
              "                                     tokenized_words  \n",
              "0      [customers, need, communicate, aperiodically]  \n",
              "1  [customs, business, development, continues, gr...  \n",
              "2  [think, team, work, hard, committed, continuou...  \n",
              "3  [overall, working, towards, customer, centric,...  \n",
              "4  [customer, centricity, growing, culture, compa...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puQszxS9-HFw",
        "colab_type": "code",
        "outputId": "ded9f1f1-9560-407b-ffb1-c64a73c28ca0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "# POS tagging\n",
        "df['POS_Tags'] = pos_tag_sents(df['comments'].apply(word_tokenize).tolist())\n",
        "df.head(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comments</th>\n",
              "      <th>tokenized_sents</th>\n",
              "      <th>tokenized_words</th>\n",
              "      <th>POS_Tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>customers need communicate aperiodically</td>\n",
              "      <td>[customers need communicate aperiodically]</td>\n",
              "      <td>[customers, need, communicate, aperiodically]</td>\n",
              "      <td>[(customers, NNS), (need, VBP), (communicate, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>customs business development continues grow ex...</td>\n",
              "      <td>[customs business development continues grow e...</td>\n",
              "      <td>[customs, business, development, continues, gr...</td>\n",
              "      <td>[(customs, NNS), (business, NN), (development,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>think team work hard committed continuous impr...</td>\n",
              "      <td>[think team work hard committed continuous imp...</td>\n",
              "      <td>[think, team, work, hard, committed, continuou...</td>\n",
              "      <td>[(think, NN), (team, NN), (work, NN), (hard, R...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>overall working towards customer centric envir...</td>\n",
              "      <td>[overall working towards customer centric envi...</td>\n",
              "      <td>[overall, working, towards, customer, centric,...</td>\n",
              "      <td>[(overall, JJ), (working, NN), (towards, NNS),...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>customer centricity growing culture company cr...</td>\n",
              "      <td>[customer centricity growing culture company c...</td>\n",
              "      <td>[customer, centricity, growing, culture, compa...</td>\n",
              "      <td>[(customer, NN), (centricity, NN), (growing, V...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            comments  \\\n",
              "0           customers need communicate aperiodically   \n",
              "1  customs business development continues grow ex...   \n",
              "2  think team work hard committed continuous impr...   \n",
              "3  overall working towards customer centric envir...   \n",
              "4  customer centricity growing culture company cr...   \n",
              "\n",
              "                                     tokenized_sents  \\\n",
              "0         [customers need communicate aperiodically]   \n",
              "1  [customs business development continues grow e...   \n",
              "2  [think team work hard committed continuous imp...   \n",
              "3  [overall working towards customer centric envi...   \n",
              "4  [customer centricity growing culture company c...   \n",
              "\n",
              "                                     tokenized_words  \\\n",
              "0      [customers, need, communicate, aperiodically]   \n",
              "1  [customs, business, development, continues, gr...   \n",
              "2  [think, team, work, hard, committed, continuou...   \n",
              "3  [overall, working, towards, customer, centric,...   \n",
              "4  [customer, centricity, growing, culture, compa...   \n",
              "\n",
              "                                            POS_Tags  \n",
              "0  [(customers, NNS), (need, VBP), (communicate, ...  \n",
              "1  [(customs, NNS), (business, NN), (development,...  \n",
              "2  [(think, NN), (team, NN), (work, NN), (hard, R...  \n",
              "3  [(overall, JJ), (working, NN), (towards, NNS),...  \n",
              "4  [(customer, NN), (centricity, NN), (growing, V...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeDHNI58E4W_",
        "colab_type": "code",
        "outputId": "1406641f-7a8b-4232-af90-90463be2d061",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "# Term frequency\n",
        "# TF = (Number of times term T appears in the particular row) / (number of terms in that row)\n",
        "df1 = (df['comments']).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\n",
        "df1.columns = ['words','tf']\n",
        "df1.head(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>words</th>\n",
              "      <th>tf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>aperiodically</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>need</td>\n",
              "      <td>2373.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>communicate</td>\n",
              "      <td>116.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>customers</td>\n",
              "      <td>918.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>business</td>\n",
              "      <td>650.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>network</td>\n",
              "      <td>109.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>internal</td>\n",
              "      <td>278.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>expand</td>\n",
              "      <td>13.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>development</td>\n",
              "      <td>595.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>continues</td>\n",
              "      <td>40.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           words      tf\n",
              "0  aperiodically     1.0\n",
              "1           need  2373.0\n",
              "2    communicate   116.0\n",
              "3      customers   918.0\n",
              "4       business   650.0\n",
              "5        network   109.0\n",
              "6       internal   278.0\n",
              "7         expand    13.0\n",
              "8    development   595.0\n",
              "9      continues    40.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YT7K6JsvF809",
        "colab_type": "code",
        "outputId": "7ccdbf6d-8233-4625-f652-0bf162fd8aec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "# Inverse Document Frequency\n",
        "# IDF = log(N/n), where, N is the total number of rows and n is the number of rows in which the word was present\n",
        "for i,word in enumerate(df1['words']):\n",
        "  df1.loc[i, 'idf'] = np.log(df1.shape[0]/(len(df1[df1['words'].str.contains(word)])))\n",
        "\n",
        "df1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>words</th>\n",
              "      <th>tf</th>\n",
              "      <th>idf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>aperiodically</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9.502637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>need</td>\n",
              "      <td>2373.0</td>\n",
              "      <td>6.170432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>communicate</td>\n",
              "      <td>116.0</td>\n",
              "      <td>7.710877</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>customers</td>\n",
              "      <td>918.0</td>\n",
              "      <td>6.506905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>business</td>\n",
              "      <td>650.0</td>\n",
              "      <td>7.200052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13390</th>\n",
              "      <td>truks</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9.502637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13391</th>\n",
              "      <td>everbod</td>\n",
              "      <td>1.0</td>\n",
              "      <td>8.809490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13392</th>\n",
              "      <td>otherskeeps</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9.502637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13393</th>\n",
              "      <td>wondering</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9.502637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13394</th>\n",
              "      <td>attitudework</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9.502637</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13395 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               words      tf       idf\n",
              "0      aperiodically     1.0  9.502637\n",
              "1               need  2373.0  6.170432\n",
              "2        communicate   116.0  7.710877\n",
              "3          customers   918.0  6.506905\n",
              "4           business   650.0  7.200052\n",
              "...              ...     ...       ...\n",
              "13390          truks     1.0  9.502637\n",
              "13391        everbod     1.0  8.809490\n",
              "13392    otherskeeps     1.0  9.502637\n",
              "13393      wondering     1.0  9.502637\n",
              "13394   attitudework     1.0  9.502637\n",
              "\n",
              "[13395 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AaOLQQhIUQR",
        "colab_type": "code",
        "outputId": "44fb173a-0976-4d58-a2fb-bca4c89a6324",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "# Term Frequency – Inverse Document Frequency (TF-IDF)\n",
        "# TF-IDF = TF * IDF\n",
        "\n",
        "df1['tf_idf'] = df1['tf'] * df1['idf']\n",
        "df1.head(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>words</th>\n",
              "      <th>tf</th>\n",
              "      <th>idf</th>\n",
              "      <th>tf_idf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>aperiodically</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9.502637</td>\n",
              "      <td>9.502637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>need</td>\n",
              "      <td>2373.0</td>\n",
              "      <td>6.170432</td>\n",
              "      <td>14642.435781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>communicate</td>\n",
              "      <td>116.0</td>\n",
              "      <td>7.710877</td>\n",
              "      <td>894.461768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>customers</td>\n",
              "      <td>918.0</td>\n",
              "      <td>6.506905</td>\n",
              "      <td>5973.338339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>business</td>\n",
              "      <td>650.0</td>\n",
              "      <td>7.200052</td>\n",
              "      <td>4680.033598</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>network</td>\n",
              "      <td>109.0</td>\n",
              "      <td>8.116342</td>\n",
              "      <td>884.681324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>internal</td>\n",
              "      <td>278.0</td>\n",
              "      <td>7.556727</td>\n",
              "      <td>2100.770004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>expand</td>\n",
              "      <td>13.0</td>\n",
              "      <td>8.116342</td>\n",
              "      <td>105.512451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>development</td>\n",
              "      <td>595.0</td>\n",
              "      <td>6.669423</td>\n",
              "      <td>3968.306946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>continues</td>\n",
              "      <td>40.0</td>\n",
              "      <td>8.116342</td>\n",
              "      <td>324.653697</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           words      tf       idf        tf_idf\n",
              "0  aperiodically     1.0  9.502637      9.502637\n",
              "1           need  2373.0  6.170432  14642.435781\n",
              "2    communicate   116.0  7.710877    894.461768\n",
              "3      customers   918.0  6.506905   5973.338339\n",
              "4       business   650.0  7.200052   4680.033598\n",
              "5        network   109.0  8.116342    884.681324\n",
              "6       internal   278.0  7.556727   2100.770004\n",
              "7         expand    13.0  8.116342    105.512451\n",
              "8    development   595.0  6.669423   3968.306946\n",
              "9      continues    40.0  8.116342    324.653697"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cW8LKJ0tJEFd",
        "colab_type": "code",
        "outputId": "73430413-6ff1-4284-8e45-de9ab13c1be1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "from nltk import RegexpParser\n",
        "from nltk import Tree\n",
        "\n",
        "# Chunking\n",
        "\n",
        "def get_continuous_chunks(text, chunk_func=ne_chunk):\n",
        "    #print(text)\n",
        "    #exit()\n",
        "    chunked = chunk_func(pos_tag(word_tokenize(text)))\n",
        "    continuous_chunk = []\n",
        "    current_chunk = []\n",
        "\n",
        "    for subtree in chunked:\n",
        "        if type(subtree) == Tree:\n",
        "            current_chunk.append(\" \".join([token for token, pos in subtree.leaves()]))\n",
        "        elif current_chunk:\n",
        "            named_entity = \" \".join(current_chunk)\n",
        "            if named_entity not in continuous_chunk:\n",
        "                continuous_chunk.append(named_entity)\n",
        "                current_chunk = []\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "    return continuous_chunk\n",
        "\n",
        "df['comments'].apply(lambda sent: get_continuous_chunks((sent)))\n",
        "df.head(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comments</th>\n",
              "      <th>tokenized_sents</th>\n",
              "      <th>tokenized_words</th>\n",
              "      <th>POS_Tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>customers need communicate aperiodically</td>\n",
              "      <td>[customers need communicate aperiodically]</td>\n",
              "      <td>[customers, need, communicate, aperiodically]</td>\n",
              "      <td>[(customers, NNS), (need, VBP), (communicate, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>customs business development continues grow ex...</td>\n",
              "      <td>[customs business development continues grow e...</td>\n",
              "      <td>[customs, business, development, continues, gr...</td>\n",
              "      <td>[(customs, NNS), (business, NN), (development,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>think team work hard committed continuous impr...</td>\n",
              "      <td>[think team work hard committed continuous imp...</td>\n",
              "      <td>[think, team, work, hard, committed, continuou...</td>\n",
              "      <td>[(think, NN), (team, NN), (work, NN), (hard, R...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>overall working towards customer centric envir...</td>\n",
              "      <td>[overall working towards customer centric envi...</td>\n",
              "      <td>[overall, working, towards, customer, centric,...</td>\n",
              "      <td>[(overall, JJ), (working, NN), (towards, NNS),...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>customer centricity growing culture company cr...</td>\n",
              "      <td>[customer centricity growing culture company c...</td>\n",
              "      <td>[customer, centricity, growing, culture, compa...</td>\n",
              "      <td>[(customer, NN), (centricity, NN), (growing, V...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            comments  \\\n",
              "0           customers need communicate aperiodically   \n",
              "1  customs business development continues grow ex...   \n",
              "2  think team work hard committed continuous impr...   \n",
              "3  overall working towards customer centric envir...   \n",
              "4  customer centricity growing culture company cr...   \n",
              "\n",
              "                                     tokenized_sents  \\\n",
              "0         [customers need communicate aperiodically]   \n",
              "1  [customs business development continues grow e...   \n",
              "2  [think team work hard committed continuous imp...   \n",
              "3  [overall working towards customer centric envi...   \n",
              "4  [customer centricity growing culture company c...   \n",
              "\n",
              "                                     tokenized_words  \\\n",
              "0      [customers, need, communicate, aperiodically]   \n",
              "1  [customs, business, development, continues, gr...   \n",
              "2  [think, team, work, hard, committed, continuou...   \n",
              "3  [overall, working, towards, customer, centric,...   \n",
              "4  [customer, centricity, growing, culture, compa...   \n",
              "\n",
              "                                            POS_Tags  \n",
              "0  [(customers, NNS), (need, VBP), (communicate, ...  \n",
              "1  [(customs, NNS), (business, NN), (development,...  \n",
              "2  [(think, NN), (team, NN), (work, NN), (hard, R...  \n",
              "3  [(overall, JJ), (working, NN), (towards, NNS),...  \n",
              "4  [(customer, NN), (centricity, NN), (growing, V...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAuxa51QPr6L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNJIQQkqsmFT",
        "colab_type": "code",
        "outputId": "720be489-cfdc-4124-c36f-7f254276f681",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "# de-tokenization\n",
        "detokenizer = []\n",
        "for i in range(len(df)):\n",
        "    t = ' '.join(df['comments'][i]) # tokenized text\n",
        "    detokenizer.append(t)\n",
        "\n",
        "df['comments'] = detokenizer\n",
        "df.head(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comments</th>\n",
              "      <th>tokenized_sents</th>\n",
              "      <th>tokenized_words</th>\n",
              "      <th>POS_Tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>c u s t o m e r s   n e e d   c o m m u n i c ...</td>\n",
              "      <td>[customers need communicate aperiodically]</td>\n",
              "      <td>[customers, need, communicate, aperiodically]</td>\n",
              "      <td>[(customers, NNS), (need, VBP), (communicate, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>c u s t o m s   b u s i n e s s   d e v e l o ...</td>\n",
              "      <td>[customs business development continues grow e...</td>\n",
              "      <td>[customs, business, development, continues, gr...</td>\n",
              "      <td>[(customs, NNS), (business, NN), (development,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>t h i n k   t e a m   w o r k   h a r d   c o ...</td>\n",
              "      <td>[think team work hard committed continuous imp...</td>\n",
              "      <td>[think, team, work, hard, committed, continuou...</td>\n",
              "      <td>[(think, NN), (team, NN), (work, NN), (hard, R...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>o v e r a l l   w o r k i n g   t o w a r d s ...</td>\n",
              "      <td>[overall working towards customer centric envi...</td>\n",
              "      <td>[overall, working, towards, customer, centric,...</td>\n",
              "      <td>[(overall, JJ), (working, NN), (towards, NNS),...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>c u s t o m e r   c e n t r i c i t y   g r o ...</td>\n",
              "      <td>[customer centricity growing culture company c...</td>\n",
              "      <td>[customer, centricity, growing, culture, compa...</td>\n",
              "      <td>[(customer, NN), (centricity, NN), (growing, V...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            comments  \\\n",
              "0  c u s t o m e r s   n e e d   c o m m u n i c ...   \n",
              "1  c u s t o m s   b u s i n e s s   d e v e l o ...   \n",
              "2  t h i n k   t e a m   w o r k   h a r d   c o ...   \n",
              "3  o v e r a l l   w o r k i n g   t o w a r d s ...   \n",
              "4  c u s t o m e r   c e n t r i c i t y   g r o ...   \n",
              "\n",
              "                                     tokenized_sents  \\\n",
              "0         [customers need communicate aperiodically]   \n",
              "1  [customs business development continues grow e...   \n",
              "2  [think team work hard committed continuous imp...   \n",
              "3  [overall working towards customer centric envi...   \n",
              "4  [customer centricity growing culture company c...   \n",
              "\n",
              "                                     tokenized_words  \\\n",
              "0      [customers, need, communicate, aperiodically]   \n",
              "1  [customs, business, development, continues, gr...   \n",
              "2  [think, team, work, hard, committed, continuou...   \n",
              "3  [overall, working, towards, customer, centric,...   \n",
              "4  [customer, centricity, growing, culture, compa...   \n",
              "\n",
              "                                            POS_Tags  \n",
              "0  [(customers, NNS), (need, VBP), (communicate, ...  \n",
              "1  [(customs, NNS), (business, NN), (development,...  \n",
              "2  [(think, NN), (team, NN), (work, NN), (hard, R...  \n",
              "3  [(overall, JJ), (working, NN), (towards, NNS),...  \n",
              "4  [(customer, NN), (centricity, NN), (growing, V...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eib6U_NRZiG0",
        "colab_type": "code",
        "outputId": "331d24ec-8774-4ae8-c407-22081d16d57f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        }
      },
      "source": [
        "# Coreference Resolution\n",
        "# Need to download and extract\n",
        "  # !wget http://nlp.stanford.edu/software/stanford-corenlp-latest.zip\n",
        "  # !unzip stanford-corenlp-latest.zip\n",
        "# After extraction goto the command promt and navigate to the extracted folder and run the below given command.\n",
        "# Then execute the program\n",
        "  # java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer\n",
        "\n",
        "# [Note] Port number is by default 9000 but it can be change.\n",
        "\n",
        "!pip install pycorenlp\n",
        "from pycorenlp import StanfordCoreNLP\n",
        "\n",
        "nlp = StanfordCoreNLP('http://localhost:9000')\n",
        "\n",
        "\n",
        "def resolve(corenlp_output):\n",
        "    \"\"\" Transfer the word form of the antecedent to its associated pronominal anaphor(s) \"\"\"\n",
        "    for coref in corenlp_output['corefs']:\n",
        "        mentions = corenlp_output['corefs'][coref]\n",
        "        antecedent = mentions[0]  # the antecedent is the first mention in the coreference chain\n",
        "        for j in range(1, len(mentions)):\n",
        "            mention = mentions[j]\n",
        "            if mention['type'] == 'PRONOMINAL':\n",
        "                # get the attributes of the target mention in the corresponding sentence\n",
        "                target_sentence = mention['sentNum']\n",
        "                target_token = mention['startIndex'] - 1\n",
        "                # transfer the antecedent's word form to the appropriate token in the sentence\n",
        "                corenlp_output['sentences'][target_sentence - 1]['tokens'][target_token]['word'] = antecedent['text']\n",
        "\n",
        "\n",
        "def print_resolved(corenlp_output):\n",
        "    \"\"\" Print the \"resolved\" output \"\"\"\n",
        "    possessives = ['hers', 'his', 'their', 'theirs']\n",
        "    for sentence in corenlp_output['sentences']:\n",
        "        for token in sentence['tokens']:\n",
        "            output_word = token['word']\n",
        "            # check lemmas as well as tags for possessive pronouns in case of tagging errors\n",
        "            if token['lemma'] in possessives or token['pos'] == 'PRP$':\n",
        "                output_word += \"'s\"  # add the possessive morpheme\n",
        "            output_word += token['after']\n",
        "            print(output_word, end='')\n",
        "\n",
        "for i in range(len(df)):\n",
        "    output = nlp.annotate(df['comments'].iloc[i], properties= {'annotators':'dcoref','outputFormat':'json','ner.useSUTime':'false'})\n",
        "    resolve(output)\n",
        "    \n",
        "    print('Original:', df['comment'].iloc[i] )\n",
        "    print('Resolved: ', end='')\n",
        "    print_resolved(output)\n",
        "    print('-'*50)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pycorenlp in /usr/local/lib/python3.6/dist-packages (0.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pycorenlp) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pycorenlp) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pycorenlp) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pycorenlp) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pycorenlp) (2020.4.5.1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m             conn = connection.create_connection(\n\u001b[0;32m--> 159\u001b[0;31m                 (self._dns_host, self.port), self.timeout, **extra_kw)\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhttplib_request_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1263\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1309\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1310\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1258\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    975\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 976\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    977\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m             raise NewConnectionError(\n\u001b[0;32m--> 168\u001b[0;31m                 self, \"Failed to establish a new connection: %s\" % e)\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x7fd756b745c0>: Failed to establish a new connection: [Errno 111] Connection refused",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    637\u001b[0m             retries = retries.increment(method, url, error=e, _pool=self,\n\u001b[0;32m--> 638\u001b[0;31m                                         _stacktrace=sys.exc_info()[2])\n\u001b[0m\u001b[1;32m    639\u001b[0m             \u001b[0mretries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_retry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_exhausted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='localhost', port=9000): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd756b745c0>: Failed to establish a new connection: [Errno 111] Connection refused',))",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pycorenlp/corenlp.py\u001b[0m in \u001b[0;36mannotate\u001b[0;34m(self, text, properties)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionError\u001b[0m: HTTPConnectionPool(host='localhost', port=9000): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd756b745c0>: Failed to establish a new connection: [Errno 111] Connection refused',))",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-2124f398188e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'comments'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'annotators'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'dcoref'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'outputFormat'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'json'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ner.useSUTime'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'false'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pycorenlp/corenlp.py\u001b[0m in \u001b[0;36mannotate\u001b[0;34m(self, text, properties)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             raise Exception('Check whether you have started the CoreNLP server e.g.\\n'\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0;34m'$ cd stanford-corenlp-full-2015-12-09/ \\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             '$ java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer')\n",
            "\u001b[0;31mException\u001b[0m: Check whether you have started the CoreNLP server e.g.\n$ cd stanford-corenlp-full-2015-12-09/ \n$ java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2mUSmQTRGL8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## TODO ##\n",
        "# Ngrams\n",
        "# Extract Collocation\n",
        "# Named Entity Recognition\n",
        "# Relationship extraction"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}