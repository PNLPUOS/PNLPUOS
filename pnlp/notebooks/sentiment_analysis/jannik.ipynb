{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\janni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if you haven't installed all the nltk assets required for all functions used in this notebook,\n",
    "# just uncomment all the lines in this cell and run it once\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "from spellchecker import SpellChecker \n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re, string, random\n",
    "\n",
    "# global variable for english stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# global variable for spell checker\n",
    "sc = SpellChecker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in dataset and structure it in a way that is beneficial for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "# currently reading in full dataset; small batch of data also available\n",
    "# path when running notebook via VS Code\n",
    "#df = pd.read_csv(\"data/pnlp_data_en.csv\", sep=\";\")\n",
    "\n",
    "# path when running notebook via jupyter\n",
    "df = pd.read_csv(\"../../data/pnlp_data_en.csv\", sep=\";\")\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split up dataframe into individual sets to continue processing\n",
    "large_department = df[df['Report Grouping'] == 'Large Department']\n",
    "ld_q1 = large_department[large_department['Question Text'] == 'Please tell us what is working well.']\n",
    "ld_q2 = large_department[large_department['Question Text'] == 'Please tell us what needs to be improved.']\n",
    "\n",
    "small_department = df[df['Report Grouping'] == 'Small Department']\n",
    "sd_q1 = small_department[small_department['Question Text'] == 'Please tell us what is working well.']\n",
    "sd_q2 = small_department[small_department['Question Text'] == 'Please tell us what needs to be improved.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Function for Data-Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(answer):\n",
    "    \"\"\"\n",
    "    Removes irrelevant information. Furthermore lemmatizes tokens and performs stopword elimination.\n",
    "    \"\"\"\n",
    "\n",
    "    tokens = pos_tag(nltk.word_tokenize(answer))\n",
    "    cleaned_tokens = []\n",
    "    # perform NER\n",
    "    # @TODO: consider also removing named entities in the cleaned\n",
    "    # string to improve classifier performance?\n",
    "    entities = nltk.chunk.ne_chunk(tokens)\n",
    "    # remove all non-entities from the list\n",
    "    # iterate backwards as to avoid problems with index after removing\n",
    "    for i in range(len(entities)-1, -1, -1):\n",
    "        if len(entities[i]) != 1:\n",
    "            del entities[i]\n",
    "        else:\n",
    "            # remove information we don't need\n",
    "            entities[i] = entities[i][0][0]\n",
    "\n",
    "    for token, tag in tokens:\n",
    "        # remove hyperlinks (specific to twitter dataset in case we want to do transfer learning)\n",
    "        # token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "        #               '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        # remove mentions of users (also specific to twitter)\n",
    "        # token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        # simplify POS tags for lemmatizer\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        \n",
    "        # perform spelling correction (very necessary in our dataset)\n",
    "        # exclude named entities\n",
    "        if token not in entities:\n",
    "            misspelled = sc.unknown([token.lower()])\n",
    "            if misspelled:\n",
    "                token = sc.correction(token.lower())\n",
    "\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        # perform stopword elimination, punctuation removal\n",
    "        # @TODO: evaluate whether stopword elimination has a positive\n",
    "        # or negative influence on classification performance\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "            \n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reopening', 'field', 'station', 'chb', 'improve', 'customer', 'service', 'message']\n"
     ]
    }
   ],
   "source": [
    "# test the function\n",
    "test_sent = \"The reopening of the field stations for CHB has improved our customer service messages.\"\n",
    "print(remove_noise(test_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heuristic to exclude abbreviations still needs to be evaluated for accuracy. Another way to implement it would be by adding NER."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Function for POS Tagging\n",
    "This was written in a context that was abandoned. I will leave the code here in case I ever need to circle back to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag(df, verbose=False, limit=-1):\n",
    "    \"\"\"\n",
    "    Extracts POS tags for every answer (important: one answer may contain multiple sentences) in a given dataframe and returns them as a list of lists.\n",
    "    Note: Function is specified for df structure of complete dataset provided by deepsight. Index adjustments might be necessary on different dfs.\n",
    "\n",
    "    df: The dataframe containing the sentences to be tagged.\n",
    "    verbose: Should the function report progress? False by default.\n",
    "    limit: If only the first n sents should be tagged, give n as limit. Tags entire df by default.\n",
    "    \"\"\"\n",
    "    # list used to store the individual lists of POS tags\n",
    "    pos_tags = []\n",
    "\n",
    "    # running index used for limit and state updates\n",
    "    i = 0\n",
    "    # set variable encoding amount of total answers to tag\n",
    "    if limit == -1:\n",
    "        end = len(df.index)\n",
    "    else:\n",
    "        end = limit\n",
    "\n",
    "    # iterature over entire dataframe\n",
    "    for row in df.iterrows():\n",
    "        # list used to store POS tags of any given answer\n",
    "        temp = []\n",
    "        # iterate over given answer\n",
    "        # adjust here for different df structures!\n",
    "        for token in nlp(row[1][2]):\n",
    "            temp.append(token.pos_)\n",
    "        # add new tags to complete list of tags\n",
    "        pos_tags.append(temp)\n",
    "\n",
    "        # defines output of verbose run of function\n",
    "        if verbose:\n",
    "            print(\"Tagging answer\", i+1, \"of\", end)\n",
    "\n",
    "        # stops function when limit is reached \n",
    "        if limit != -1:\n",
    "            if i == limit - 1: return(pos_tags)\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "    return(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the POS tag function\n",
    "taglist = pos_tag(ld_q1, verbose=True, limit=10)\n",
    "print(ld_q1['Comments'][0])\n",
    "print(taglist[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Function for Lemmatization\n",
    "This was written in a context that was abandoned. I will leave the code here in case I ever need to circle back to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(df, verbose=False, limit=-1):\n",
    "    \"\"\"\n",
    "    Extracts lemmas for every answer (important: one answer may contain multiple sentences) in a given dataframe and returns them as a list of lists.\n",
    "    Note: Function is specified for df structure of complete dataset provided by deepsight. Index adjustments might be necessary on different dfs.\n",
    "\n",
    "    df: The dataframe containing the sentences to be lemmatized.\n",
    "    verbose: Should the function report progress? False by default.\n",
    "    limit: If only the first n sents should be lemmatized, give n as limit. Lemmatizes entire df by default.\n",
    "    \"\"\"\n",
    "    # list used to store the individual lists of POS tags\n",
    "    lemmas = []\n",
    "\n",
    "    # running index used for limit and state updates\n",
    "    i = 0\n",
    "    # set variable encoding amount of total answers to tag\n",
    "    if limit == -1:\n",
    "        end = len(df.index)\n",
    "    else:\n",
    "        end = limit\n",
    "\n",
    "    # iterature over entire dataframe\n",
    "    for row in df.iterrows():\n",
    "        # list used to store POS tags of any given answer\n",
    "        temp = []\n",
    "        # iterate over given answer\n",
    "        # adjust here for different df structures!\n",
    "        for token in nlp(row[1][2]):\n",
    "            temp.append(token.lemma_)\n",
    "        # add new tags to complete list of tags\n",
    "        lemmas.append(temp)\n",
    "\n",
    "        # defines output of verbose run of function\n",
    "        if verbose:\n",
    "            print(\"Lemmatizing answer\", i+1, \"of\", end)\n",
    "\n",
    "        # stops function when limit is reached \n",
    "        if limit != -1:\n",
    "            if i == limit - 1: return(lemmas)\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "    return(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the POS tag function\n",
    "lemmalist = lemmatize(ld_q1, verbose=True, limit=10)\n",
    "print(ld_q1['Comments'][0])\n",
    "print(lemmalist[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
